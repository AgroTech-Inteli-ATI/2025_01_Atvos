# ğŸ§© **DocumentaÃ§Ã£o da API â€“ Agro-Server**

## ğŸ“˜ **VisÃ£o Geral**

O **Agro-Server** Ã© uma API desenvolvida em **Django** com o objetivo de oferecer uma interface modularizada para manipulaÃ§Ã£o de dados no **Google BigQuery** e no **Google Cloud Storage**.
A API implementa mÃ©todos padronizados para:

* **Inserir** registros
* **Atualizar** registros
* **Remover** registros
* **Fazer download** de arquivos

Essas funcionalidades permitem integrar sistemas agrÃ­colas com infraestrutura em nuvem de forma escalÃ¡vel e segura.

---

## âš™ï¸ **Arquitetura e Estrutura de Pastas**

```
Agro-Server/
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ bigquery_views.py      # Endpoints relacionados ao BigQuery
â”‚   â”œâ”€â”€ storage_views.py       # Endpoints relacionados ao Cloud Storage
â”‚   â”œâ”€â”€ settings.py            # ConfiguraÃ§Ãµes do Django (importa variÃ¡veis do .env)
â”‚   â”œâ”€â”€ urls.py                # DefiniÃ§Ã£o das rotas da API
â”‚   â””â”€â”€ wsgi.py                # Ponto de entrada do servidor Django
â”‚
â”œâ”€â”€ clients/
â”‚   â”œâ”€â”€ bigquery_client.py     # Cliente responsÃ¡vel por se conectar e manipular dados no BigQuery
â”‚   â”œâ”€â”€ storage_client.py      # Cliente responsÃ¡vel por se conectar ao Cloud Storage
â”‚   â””â”€â”€ key.json               # Credenciais de acesso do Google Cloud
â”‚
â”œâ”€â”€ manage.py                  # Comando administrativo do Django
â”œâ”€â”€ .env                       # VariÃ¡veis de ambiente (configuraÃ§Ãµes e chaves)
â”œâ”€â”€ db.sqlite3                 # Banco de dados local do Django
â”œâ”€â”€ requirements.txt           # DependÃªncias do projeto
â””â”€â”€ venv/                      # Ambiente virtual Python
```

---

## â˜ï¸ **ServiÃ§os Utilizados**

### ğŸŸ¢ **Google BigQuery**

O **BigQuery** Ã© o *Data Warehouse* do Google Cloud voltado para anÃ¡lises em larga escala.
Ele permite **armazenar e consultar grandes volumes de dados usando SQL**, com alta performance e escalabilidade.

Nesta API, o BigQuery Ã© utilizado para:

* Inserir novos registros em tabelas especÃ­ficas
* Atualizar registros existentes
* Remover registros
* Executar consultas e retornar dados para o cliente

> O mÃ³dulo responsÃ¡vel Ã© `clients/bigquery_client.py`
> As rotas estÃ£o em `api/bigquery_views.py`

---

### ğŸŸ£ **Google Cloud Storage**

O **Cloud Storage** Ã© o serviÃ§o de **armazenamento de objetos** (blobs) do Google Cloud.
Permite salvar e gerenciar arquivos como imagens, documentos e dados de backup.

Nesta API, o Cloud Storage Ã© utilizado para:

* Fazer **upload** de arquivos locais para buckets na nuvem
* Fazer **download** de arquivos armazenados
* **Remover** arquivos de buckets

> O mÃ³dulo responsÃ¡vel Ã© `clients/storage_client.py`
> As rotas estÃ£o em `api/storage_views.py`

---

## ğŸ§  **Rotas DisponÃ­veis**

### ğŸ“Š **BigQuery**

| MÃ©todo HTTP | Endpoint               | FunÃ§Ã£o                            |
| ----------- | ---------------------- | --------------------------------- |
| `POST`      | `/bigquery/inserir/`   | Insere um novo registro na tabela |
| `PUT`       | `/bigquery/atualizar/` | Atualiza dados existentes         |
| `DELETE`    | `/bigquery/remover/`   | Remove um registro da tabela      |
| `GET`       | `/bigquery/download/`  | Exporta dados do BigQuery         |

---

### ğŸ“¦ **Cloud Storage**

| MÃ©todo HTTP | Endpoint             | FunÃ§Ã£o                                |
| ----------- | -------------------- | ------------------------------------- |
| `POST`      | `/storage/inserir/`  | Faz upload de um arquivo local        |
| `DELETE`    | `/storage/remover/`  | Remove um arquivo de um bucket        |
| `GET`       | `/storage/download/` | Faz o download de um arquivo da nuvem |

---

## ğŸ” **ConfiguraÃ§Ãµes e Credenciais**

As credenciais de acesso (`key.json`) e as variÃ¡veis de ambiente (`.env`) estÃ£o disponÃ­veis **no grupo do WhatsApp da ATVOS**.
Esses arquivos devem ser colocados nas seguintes localizaÃ§Ãµes:

```
/Agro-Server/clients/key.json
/Agro-Server/.env
```

O arquivo `.env` contÃ©m informaÃ§Ãµes como:

```
GOOGLE_APPLICATION_CREDENTIALS=clients/key.json
BIGQUERY_PROJECT_ID=nome-do-projeto
STORAGE_BUCKET_NAME=nome-do-bucket
DJANGO_SECRET_KEY=chave_django
DEBUG=True
```

---

## ğŸ§© **Ambiente Virtual (venv)**

O uso do **venv** garante que todas as dependÃªncias do projeto fiquem isoladas do sistema operacional.

### 1ï¸âƒ£ Criar o ambiente virtual

```bash
python -m venv venv
```

### 2ï¸âƒ£ Ativar o ambiente

* **Linux/macOS:**

  ```bash
  source venv/bin/activate
  ```
* **Windows:**

  ```bash
  venv\Scripts\activate
  ```

### 3ï¸âƒ£ Instalar as dependÃªncias

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Rodar o servidor Django

```bash
python manage.py runserver
```

---

## ğŸ§  **Fluxo de Funcionamento**

1. O usuÃ¡rio envia uma requisiÃ§Ã£o HTTP para uma das rotas (`/bigquery/...` ou `/storage/...`).
2. A view correspondente (`bigquery_views.py` ou `storage_views.py`) valida os dados recebidos.
3. A view utiliza o cliente apropriado (`bigquery_client.py` ou `storage_client.py`) para interagir com o Google Cloud.
4. O cliente usa as credenciais em `key.json` e as variÃ¡veis do `.env` para autenticar a operaÃ§Ã£o.
5. A resposta (sucesso ou erro) Ã© retornada em formato JSON para o usuÃ¡rio.

---

## ğŸ§¾ **Requisitos do Sistema**

* **Python 3.10+**
* **Django 5+**
* **google-cloud-bigquery**
* **google-cloud-storage**
* **python-dotenv**

> Todas as dependÃªncias estÃ£o listadas no arquivo `requirements.txt`.

---
## ETL
 ### VisÃ£o Geral

  O Agro-Server Ã© um backend construÃ­do com Django, projetado para gerenciar e processar dados de viagens. Seu principal componente Ã© um pipeline de ETL (ExtraÃ§Ã£o, TransformaÃ§Ã£o e
  Carga) que coleta dados do ambiente local, os processa e os carrega no Google BigQuery para anÃ¡lise.

  Este documento serve como um guia completo para configurar, executar e entender a arquitetura do projeto.

 ### Estrutura do Projeto

  Entender a organizaÃ§Ã£o dos arquivos Ã© fundamental para trabalhar no projeto:

```
  1 /
  2 â”œâ”€â”€ api/                  # App Django principal
  3 â”‚   â”œâ”€â”€ migrations/       # MigraÃ§Ãµes do banco de dados
  4 â”‚   â”œâ”€â”€ management/       # Comandos de gerenciamento customizados
  5 â”‚   â”‚   â””â”€â”€ commands/
  6 â”‚   â”‚       â””â”€â”€ run_etl.py  # Orquestrador do pipeline ETL
  7 â”‚   â”œâ”€â”€ models.py         # Modelos de dados do Django (ex: Viagem)
  8 â”‚   â”œâ”€â”€ settings.py       # ConfiguraÃ§Ãµes do Django
  9 â”‚   â””â”€â”€ urls.py           # Rotas da API
 10 â”œâ”€â”€ clients/              # Clientes para serviÃ§os externos
 11 â”‚   â”œâ”€â”€ __init__.py       # Inicializa e exporta os clientes
 12 â”‚   â”œâ”€â”€ bigquery_client.py# Cliente para interagir com o Google BigQuery
 13 â”‚   â”œâ”€â”€ storage_client.py # Cliente para interagir com o Google Cloud Storage
 14 â”‚   â””â”€â”€ etl/              # MÃ³dulos especÃ­ficos do ETL
 15 â”‚       â”œâ”€â”€ extractor.py    # LÃ³gica de extraÃ§Ã£o de dados
 16 â”‚       â”œâ”€â”€ transformer.py  # LÃ³gica de transformaÃ§Ã£o de dados
 17 â”‚       â”œâ”€â”€ loader.py       # LÃ³gica de carregamento de dados
 18 â”‚       â””â”€â”€ auditor.py      # LÃ³gica de auditoria dos dados
 19 â”œâ”€â”€ logs/                 # DiretÃ³rio para arquivos de log
 20 â”‚   â””â”€â”€ etl.log           # Log das execuÃ§Ãµes do ETL
 21 â”œâ”€â”€ .env                  # Arquivo para variÃ¡veis de ambiente (NÃƒO versionado)
 22 â”œâ”€â”€ manage.py             # UtilitÃ¡rio de linha de comando do Django
 23 â””â”€â”€ requirements.txt      # Lista de dependÃªncias Python
```
  ### ConfiguraÃ§Ã£o do Ambiente de Desenvolvimento

  Siga estes passos para configurar o ambiente localmente.

  1. PrÃ©-requisitos

   - Python 3.10 ou superior.
   - Acesso a um projeto no Google Cloud com BigQuery e Cloud Storage ativados.
   - Uma conta de serviÃ§o do Google Cloud com permissÃµes de acesso (BigQuery Data Editor, Storage Object Admin) e o arquivo de credenciais JSON.

  2. Ambiente Virtual (venv)

  Ã‰ uma boa prÃ¡tica isolar as dependÃªncias do projeto.

   2.1. Crie o ambiente virtual:
      No diretÃ³rio raiz do projeto, execute:
        python3 -m venv .venv

   2.2. Ative o ambiente virtual:
       - No Linux/macOS:
            source .venv/bin/activate
       - No Windows:
            .venv\Scripts\activate
      ApÃ³s a ativaÃ§Ã£o, seu terminal deve exibir (.venv) no inÃ­cio da linha.

  ### InstalaÃ§Ã£o de DependÃªncias

  Com o ambiente virtual ativado, instale todas as bibliotecas necessÃ¡rias:

   1 pip install -r requirements.txt

  ### VariÃ¡veis de Ambiente (.env)

  Crie um arquivo chamado .env na raiz do projeto. Este arquivo armazena configuraÃ§Ãµes sensÃ­veis e especÃ­ficas do seu ambiente.

  ConteÃºdo do arquivo `.env`:

    1 # Credenciais do Google Cloud
    2 # Caminho absoluto para o arquivo JSON da sua conta de serviÃ§o.
    3 GOOGLE_APPLICATION_CREDENTIALS="/caminho/completo/para/seu/arquivo-de-credenciais.json"
    4 
    5 # ConfiguraÃ§Ãµes do Django
    6 SECRET_KEY="gere-uma-chave-aleatoria-e-forte-aqui"
    7 DEBUG=True
    8 
    9 # ConfiguraÃ§Ãµes do Google Cloud
   10 BIGQUERY_PROJECT_ID="seu-id-de-projeto-no-gcp"
   11 BIGQUERY_DATASET_NAME="nome_do_seu_dataset_no_bigquery"
   12 CLOUD_STORAGE_BUCKET="nome_do_seu_bucket_no_gcs"

  Detalhes das variÃ¡veis:

   - GOOGLE_APPLICATION_CREDENTIALS: O caminho absoluto para o arquivo JSON que vocÃª baixou ao criar uma conta de serviÃ§o no Google Cloud.
   - SECRET_KEY: Uma chave secreta para seguranÃ§a do Django. VocÃª pode gerar uma online ou usar uma string longa e aleatÃ³ria.
   - DEBUG: True para ambiente de desenvolvimento (mostra erros detalhados) e False para produÃ§Ã£o.
   - BIGQUERY_PROJECT_ID: O ID do seu projeto no Google Cloud Platform.
   - BIGQUERY_DATASET_NAME: O nome do dataset (conjunto de dados) no BigQuery onde a tabela viagens_cleaned serÃ¡ criada.
   - CLOUD_STORAGE_BUCKET: O nome do bucket no Google Cloud Storage que serÃ¡ usado como Ã¡rea de preparaÃ§Ã£o (staging) para os arquivos CSV.

  ### MigraÃ§Ãµes do Banco de Dados

  O projeto usa um banco de dados SQLite local para armazenar os dados antes do ETL. Para criar as tabelas necessÃ¡rias, execute:

   1 # Cria os arquivos de migraÃ§Ã£o a partir dos modelos
   2 python3 manage.py makemigrations api
   3 
   4 # Aplica as migraÃ§Ãµes ao banco de dados
   5 python3 manage.py migrate

  ### Executando o Pipeline ETL

  O coraÃ§Ã£o do projeto Ã© o comando run_etl.

  ### Como Executar

  Para iniciar o pipeline completo, execute o seguinte comando no seu terminal (com o venv ativado):

   python3 manage.py run_etl

  O comando irÃ¡ executar todas as etapas do ETL em sequÃªncia e registrarÃ¡ o progresso no terminal e no arquivo logs/etl.log.

  ### Funcionamento Detalhado do ETL

  O pipeline Ã© orquestrado pelo arquivo api/management/commands/run_etl.py e dividido em quatro etapas principais:

  Etapa 1: ExtraÃ§Ã£o (`clients/etl/extractor.py`)

   - O que faz? Coleta todos os registros do modelo Viagem do banco de dados local (SQLite).
   - Como faz? Usa a funÃ§Ã£o apps.get_model('api', 'Viagem') do Django para acessar o modelo dinamicamente e, em seguida, converte o QuerySet de viagens em um DataFrame do pandas.
   - SaÃ­da: Um DataFrame contendo os dados brutos das viagens.

  Etapa 2: TransformaÃ§Ã£o (`clients/etl/transformer.py`)

   - O que faz? Limpa, enriquece e prepara os dados para anÃ¡lise.
   - Como faz? Recebe o DataFrame da etapa de extraÃ§Ã£o e aplica as seguintes transformaÃ§Ãµes:
       1. Remove registros duplicados baseados no id.
       2. Converte as colunas de odÃ´metro para formato numÃ©rico.
       3. Calcula a coluna km_variavel (odometro_fim - odometro_inicio).
       4. Calcula a coluna km_esperado (uma estimativa usada para auditoria).
       5. Calcula a coluna custos_consolidados (custos_fixos + custos_variaveis).
       6. Tratamento de Erro: Se o DataFrame de entrada estiver vazio, a funÃ§Ã£o retorna imediatamente para evitar erros nas etapas seguintes.
   - SaÃ­da: Um DataFrame transformado e pronto para ser carregado.

  Etapa 3: Carga (`clients/etl/loader.py`)

   - O que faz? Carrega os dados transformados no Google BigQuery.
   - Como faz? O processo Ã© feito em duas fases para otimizaÃ§Ã£o:
       1. Upload para o Cloud Storage: O DataFrame transformado Ã© primeiro salvo como um arquivo CSV em um diretÃ³rio temporÃ¡rio local. Em seguida, esse arquivo Ã© enviado para um bucket no
          Google Cloud Storage. Isso serve como uma Ã¡rea de preparaÃ§Ã£o (staging area).
       2. Carga no BigQuery: A funÃ§Ã£o instrui o BigQuery a importar os dados diretamente do arquivo CSV no Cloud Storage para a tabela viagens_cleaned. A tabela Ã© configurada com
          WRITE_TRUNCATE, o que significa que a cada execuÃ§Ã£o do ETL, a tabela Ã© apagada e recriada com os novos dados.
   - SaÃ­da: O URI (caminho) do arquivo CSV no Google Cloud Storage.

  Etapa 4: Auditoria (`clients/etl/auditor.py`)

   - O que faz? Analisa os dados transformados em busca de inconsistÃªncias.
   - Como faz?
       1. Calcula a coluna flag_divergencia, que sinaliza (True) se a quilometragem real (km_variavel) diverge mais de 10% da esperada (km_esperado).
       2. Calcula um score de conformidade para cada registro.
       3. Tratamento de Erro: Assim como o transformador, esta funÃ§Ã£o tambÃ©m verifica se o DataFrame estÃ¡ vazio antes de prosseguir.
   - SaÃ­da: O DataFrame final, enriquecido com as colunas de auditoria.

  Ao final, o comando run_etl exibe um resumo da execuÃ§Ã£o, incluindo o nÃºmero de registros processados, inconsistÃªncias encontradas e o tempo total. o Servidor Backend

  Embora o foco seja o ETL, o projeto tambÃ©m Ã© um backend Django. Para iniciar o servidor de desenvolvimento:

   1 python3 manage.py runserver

  O servidor estarÃ¡ acessÃ­vel em http://127.0.0.1:8000/.

  ---

## âœ‰ï¸ **Contato**

Em caso de dÃºvidas sobre configuraÃ§Ã£o ou chaves de acesso, entre em contato pelo grupo da **ATVOS no WhatsApp**, onde estÃ£o disponÃ­veis o `.env` e o `key.json`.

